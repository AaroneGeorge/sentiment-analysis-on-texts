{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1svD1SXUFgz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po8aywGMUXuN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\");\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zee8qeTAsaIW"
      },
      "source": [
        "Text Cleaning:\n",
        "\n",
        "Lowercasing: Convert all text to lowercase to ensure consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm6RSEmVqdgD"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaNnhmkTsi4z"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zNJulnHskxL"
      },
      "source": [
        "  Remove HTML Tags: Some reviews may contain HTML tags. Remove them using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hja76Zr7ss02"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].str.replace('<br /><br />', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiYlq2Rusvmt"
      },
      "outputs": [],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KhS9LN7s5LO"
      },
      "source": [
        "Remove Special Characters and Numbers: Remove non-alphabetical characters and numbers, as they may not be relevant for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw-oMEr4s0du"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].str.replace('[^a-zA-Z\\s]', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH4mbVBfs8D1"
      },
      "outputs": [],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1AVla_5s7yF"
      },
      "source": [
        "Tokenization:\n",
        "\n",
        "Tokenization is the process of splitting text into individual words or tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsfEWoKltbxO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJqHwleQthKj"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMUWW3sxtu90"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP87u8GhtsLN"
      },
      "source": [
        "Stopword Removal:\n",
        "Stopwords are common words (e.g., \"and,\" \"the,\" \"is\") that often don't carry much meaning in sentiment analysis. You can remove them to reduce noise in your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72bryXpjtyJN"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['review'] = df['review'].apply(lambda x: [word for word in x if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXEllFVdt609"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zEVQAz7uUl2"
      },
      "source": [
        "Lemmatization or Stemming (Optional):\n",
        "Lemmatization and stemming reduce words to their base or root form. This can help in reducing the dimensionality of your data and improving model performance.\n",
        "\n",
        "Stemming:\n",
        "\n",
        "Stemming involves chopping off the ends of words to remove prefixes or suffixes.\n",
        "The goal is to reduce words to their \"stem\" or \"root\" form.\n",
        "For example, the word \"jumping\" would be stemmed to \"jump,\" and \"running\" would become \"run.\"\n",
        "Stemming is a simple and fast method, but it may not always produce real words, and the resulting stems may not be valid in all contexts.\n",
        "\n",
        "Lemmatization:\n",
        "\n",
        "Lemmatization is a more sophisticated approach that reduces words to their \"lemma\" or \"base form.\"\n",
        "It takes into account the word's grammatical meaning and tries to produce a valid word.\n",
        "For example, the word \"better\" would be lemmatized to \"good,\" and \"went\" would become \"go.\"\n",
        "Lemmatization is a bit slower than stemming because it considers the word's context and meaning, but it often produces more accurate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuwDLcJWv-9e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6E8cQV0uUXW"
      },
      "outputs": [],
      "source": [
        "#Lemmatization (using WordNet Lemmatizer from NLTK)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['review'] = df['review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZN_Yy43wRWd"
      },
      "outputs": [],
      "source": [
        "#Stemming (using Porter Stemmer from NLTK)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "df['review'] = df['review'].apply(lambda x: [stemmer.stem(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "netXccQAxBaF"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1PKe4lIxMH1"
      },
      "outputs": [],
      "source": [
        "#Join Tokens Back into Sentences\n",
        "\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_xqvwx-xkwO"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtb4m2XzVOk"
      },
      "outputs": [],
      "source": [
        "# for future use\n",
        "df.to_csv('preprocessed_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvAe8I472HaF"
      },
      "source": [
        "Splitting the Data:\n",
        "\n",
        "Divide your dataset into two parts: one for training your sentiment analysis model and the other for testing its performance. A common split is 80% of the data for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CufzzYqO0HdO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQdsLCT341hO"
      },
      "source": [
        "Feature Extraction\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): This technique measures the importance of each word in a document relative to a collection of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inWFpuBF4HQn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed dataset\n",
        "df = pd.read_csv('preprocessed_dataset.csv')\n",
        "\n",
        "# Option 1: TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # adjust max_features as needed\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "\n",
        "# X_tfidf now contains TF-IDF feature vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hrGgTar9sjA"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(X_tfidf, 'X_tfidf.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVCaNIGM5mjl"
      },
      "source": [
        "**MODEL SELECTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dYWKDHo58uF"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPDieNQb51H9"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets (X_tfidf is your TF-IDF feature matrix)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Naive Bayes model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "report_nb = classification_report(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}\")\n",
        "print(report_nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGatnXC355Vt"
      },
      "source": [
        "**LOGISTIC REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tjy1uaB2KB1"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets (X_tfidf is the TF-IDF feature matrix)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the logistic regression model\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}\")\n",
        "print(report_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGPd_nzT9OWd"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(lr_model, 'model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYIUfJk8PLN"
      },
      "source": [
        "TESTING ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvPzjbg78QdW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# Load the preprocessed dataset and model\n",
        "df = pd.read_csv('preprocessed_dataset.csv')\n",
        "# Load the TF-IDF vectorizer and features\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Load the vectorizer\n",
        "X_tfidf = joblib.load('X_tfidf.pkl')  # Load the TF-IDF features\n",
        "y = df['sentiment']\n",
        "\n",
        "# Load the trained Logistic Regression model\n",
        "model = joblib.load('model.pkl')\n",
        "\n",
        "# Input text to be analyzed\n",
        "input_text = input(\"Enter your text: \")\n",
        "\n",
        "\n",
        "# Transform the input text into TF-IDF features\n",
        "input_features = tfidf_vectorizer.transform([input_text])\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "prediction = model.predict(input_features)\n",
        "\n",
        "\n",
        "print(f\"Sentiment Analysis Result: {prediction[0]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
